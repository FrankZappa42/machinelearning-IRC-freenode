Contents:
<!-- TOC can be generated manually using https://ecotrust-canada.github.io/markdown-toc/ -->
- [Neural networks](#neural-networks)
- [Deep learning](#deep-learning)
  * [Overview](#overview)
    + [Terminology](#terminology)
  * [Tutorials](#tutorials)
  * [Theory](#theory)
    + [Analysis](#analysis)
    + [Expressivity](#expressivity)
    + [Limitations](#limitations)
  * [Architecture](#architecture)
  * [Practice](#practice)
  * [Software](#software)
    + [Comparison](#comparison)
    + [TensorFlow](#tensorflow)
  * [Alternatives](#alternatives)
- [Deep convolutional networks](#deep-convolutional-networks)
  * [Overview](#overview-1)
  * [Theory](#theory-1)
    + [Analysis](#analysis-1)
- [Deep reinforcement learning](#deep-reinforcement-learning)
  * [Overview](#overview-2)
  * [Analysis](#analysis-2)
  * [Alternatives](#alternatives-1)

## Neural networks
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
* [arXiv] [Neural Networks for Beginners. A fast implementation in Matlab, Torch, TensorFlow (2017)](https://arxiv.org/abs/1703.05298)
* [arXiv] [Understanding Convolutional Neural Networks (2016)](https://arxiv.org/abs/1605.09081)
### Limitations
* [arXiv] [On the difficulty of training Recurrent Neural Networks (2013)](https://arxiv.org/abs/1211.5063)

## Deep learning
### Overview
* [Deep learning (2015)](http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html) ([pdf](http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf))
* [arXiv] [Deep Learning in Neural Networks: An Overview (2014)](https://arxiv.org/abs/1404.7828) (link-dense)
* [arXiv] [On the Origin of Deep Learning (2017)](https://arxiv.org/abs/1702.07800)
#### Terminology
* [arXiv] [Semantics, Representations and Grammars for Deep Learning (2015)](https://arxiv.org/abs/1509.08627)
### Tutorials
* [arXiv] [A Tutorial on Deep Neural Networks for Intelligent Systems (2016)](https://arxiv.org/abs/1603.07249)
* [arXiv] [Tutorial on Answering Questions about Images with Deep Learning (2016)](https://arxiv.org/abs/1610.01076)
### Theory
* [arXiv] [Understanding deep learning requires rethinking generalization (2017)](https://arxiv.org/abs/1611.03530)
* [arXiv] [On Generalization and Regularization in Deep Learning (2017)](https://arxiv.org/abs/1704.01312)
* [arXiv] [Why does deep and cheap learning work so well? (2017)](https://arxiv.org/abs/1608.08225v2)
* [arXiv] [Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of Dimensionality: a Review (2017)](https://arxiv.org/abs/1611.00740)
* [arXiv] [Geometry of Optimization and Implicit Regularization in Deep Learning (2017)](https://arxiv.org/abs/1705.03071)
* [arXiv] [Deep Learning and the Information Bottleneck Principle (2015)](https://arxiv.org/abs/1503.02406)
#### Analysis
* [arXiv] [Opening the Black Box of Deep Neural Networks via Information (2017)](https://arxiv.org/abs/1703.00810)
* [arXiv] [Why does Deep Learning work? - A perspective from Group Theory (2015)](https://arxiv.org/abs/1412.6621)
#### Expressivity
* [arXiv] [On the Expressive Power of Deep Learning: A Tensor Analysis (2016)](https://arxiv.org/abs/1509.05009)
* [arXiv] [On the Expressive Power of Deep Neural Networks (2017)](https://arxiv.org/abs/1606.05336)
* [arXiv] [Exponential expressivity in deep neural networks through transient chaos (2016)](https://arxiv.org/abs/1606.05340)
#### Limitations
* [arXiv] [Failures of Gradient-Based Deep Learning (2017)](https://arxiv.org/abs/1703.07950)
* [arXiv] [The Limitations of Deep Learning in Adversarial Settings (2015)](https://arxiv.org/abs/1511.07528) (also see descendent cites)
### Architecture
* [arXiv] [On architectural choices in deep learning: From network structure to gradient convergence and parameter estimation (2017)](https://arxiv.org/abs/1702.08670)
#### Automation
* [arXiv] [Designing Neural Network Architectures using Reinforcement Learning (2017)](https://arxiv.org/abs/1611.02167)
* [arXiv] [DeepArchitect: Automatically Designing and Training Deep Architectures (2017)](https://arxiv.org/abs/1704.08792)
* [arXiv] [Evolving Deep Neural Networks (2017)](https://arxiv.org/abs/1703.00548)
### Practice
* [arXiv] [An Analysis of Deep Neural Network Models for Practical Applications (2017)](https://arxiv.org/abs/1605.07678)
* [arXiv] [Best Practices for Applying Deep Learning to Novel Applications (2017)](https://arxiv.org/abs/1704.01568)
* [arXiv] [Fathom: Reference Workloads for Modern Deep Learning Methods (2016)](https://arxiv.org/abs/1608.06581)
* [arXiv] [Tricks from Deep Learning (2016)](https://arxiv.org/abs/1611.03777)
* [arXiv] [Practical recommendations for gradient-based training of deep architectures (2012)](https://arxiv.org/abs/1206.5533)
* [The Black Magic of Deep Learning - Tips and Tricks for the practitioner (2017)](https://nmarkou.blogspot.fr/2017/02/the-black-magic-of-deep-learning-tips.html)
### Software
#### Comparison
* [arXiv] [Comparative Study of Deep Learning Software Frameworks (2016)](https://arxiv.org/abs/1511.06435)
* [arXiv] [Benchmarking State-of-the-Art Deep Learning Software Tools (2017)](https://arxiv.org/abs/1608.07249)
#### TensorFlow
* [arXiv] [TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems (2016)](https://arxiv.org/abs/1603.04467)
* [arXiv] [A Tour of TensorFlow (2016)](https://arxiv.org/abs/1610.01178)
* [arXiv] [Should I use TensorFlow (2016)](https://arxiv.org/abs/1611.08903)
### Alternatives
* [arXiv] [Deep Forest: Towards An Alternative to Deep Neural Networks (2017)](https://arxiv.org/abs/1702.08835)

## Deep convolutional networks
### Overview
* [arXiv] [Understanding Deep Convolutional Networks (2016)](https://arxiv.org/abs/1601.04920)
* [arXiv] [A guide to convolution arithmetic for deep learning (2016)](https://arxiv.org/abs/1603.07285)
### Theory
* [arXiv] [Do Deep Convolutional Nets Really Need to be Deep and Convolutional? (2017)](https://arxiv.org/abs/1603.05691)
* [arXiv] [Theory II: Landscape of the Empirical Risk in Deep Learning (2017)](https://arxiv.org/abs/1703.09833)
#### Analysis
* [arXiv] [Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions (2017)](https://arxiv.org/abs/1705.02302)
* [arXiv] [Towards Better Analysis of Deep Convolutional Neural Networks (2016)](https://arxiv.org/abs/1604.07043)
### Design
* [arXiv] [Deep Convolutional Neural Network Design Patterns (2016)](https://arxiv.org/abs/1611.00847)

## Deep reinforcement learning
### Overview
* [arXiv] [Deep Reinforcement Learning: An Overview (2017)](https://arxiv.org/abs/1701.07274)
### Analysis
* [arXiv] [Graying the black box: Understanding DQNs (2017)](https://arxiv.org/abs/1602.02658)
### Alternatives
* [arXiv] [Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017)](https://arxiv.org/abs/1703.03864)
